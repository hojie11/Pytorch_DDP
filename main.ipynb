{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Distributed Data parallel(DDP) processing tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import pytorch module for do DDP processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Design own your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simpleNN(\n",
      "  (fc0): FClayer(\n",
      "    (main): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc1): FClayer(\n",
      "    (main): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc2): FClayer(\n",
      "    (main): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc3): FClayer(\n",
      "    (main): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc4): FClayer(\n",
      "    (main): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc5): FClayer(\n",
      "    (main): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc6): FClayer(\n",
      "    (main): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc7): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class FClayer(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.main = nn.Sequential(nn.Linear(dim_in, dim_out),\n",
    "                                  nn.ReLU())\n",
    "\n",
    "    def forward(self, x):   \n",
    "        return self.main(x)\n",
    "\n",
    "class simpleNN(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, hidden_dim, num_layers) -> None:\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        features_list = [dim_in] + [hidden_dim] * (num_layers - 1) + [dim_out]\n",
    "\n",
    "        for idx in range(num_layers):\n",
    "            _in = features_list[idx]\n",
    "            _out = features_list[idx + 1]\n",
    "            if idx != (num_layers - 1):\n",
    "                layer = FClayer(_in, _out)\n",
    "            else:\n",
    "                layer = nn.Linear(_in, _out)\n",
    "            setattr(self, f'fc{idx}', layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for idx in range(self.num_layers):\n",
    "            layer = getattr(self, f'fc{idx}')\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "model = simpleNN(dim_in=64, dim_out=4, hidden_dim=256, num_layers=8)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    args.world_size = n_gpus\n",
    "\n",
    "    if args.use_ddp: # a flag for using Pytorch DDP\n",
    "        args.world_size = n_gpus * args.world_size\n",
    "        args.num_workers = n_gpus * 4 # it's common to multiply 4 with how many gpu you have\n",
    "        args.batch_size = n_gpus * args.batch_size # to split batch per each gpu\n",
    "        args.val_batch_size = n_gpus * args.val_batch_size # same above\n",
    "        mp.spawn(main_worker, nprocs=n_gpus, args=(n_gpus, args)) # pytorch multiprocessing spawn\n",
    "    else: # a flag for not using Pytorch DDP\n",
    "        args.gpu = 0 # set first gpu id\n",
    "        main_worker(args.gpu, n_gpus, args)\n",
    "\n",
    "def main_worker(gpu, ngpus_per_node, args):\n",
    "    args.gpu = gpu\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "\n",
    "    if args.gpu is not None:\n",
    "        print(\"Use GPU: {} for training\".format(args.gpu))\n",
    "\n",
    "    if args.distributed:\n",
    "        if args.is_distributed:\n",
    "            args.rank = args.rank * ngpus_per_node + gpu\n",
    "        dist.init_process_group(backend='nccl', # 'gloo', 'mpi', or 'nccl', check out they have different capabilities\n",
    "                                init_method='tcp://127.0.0.1:88', #{localhost}:PORT\n",
    "                                world_size=args.world_size, \n",
    "                                rank=args.rank)\n",
    "\n",
    "    # load dataset\n",
    "    transform = transforms.Compose([transforms.Resize([256, 256]),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                                        std=[0.5, 0.5, 0.5])])\n",
    "    dataset = ImageFolder(\"your directory\", transform=transform)\n",
    "\n",
    "    sampler = DistributedSampler(dataset=dataset, shuffle=True) # split train data per each process\n",
    "    dataloader = DataLoader(dataset, \n",
    "                            batch_size=int(args.batch_size / args.world_size),\n",
    "                            shuffle=False,\n",
    "                            num_workers=int(args.num_workers / args.world_size),\n",
    "                            sampler=sampler,\n",
    "                            pin_memory=True)\n",
    "    \n",
    "    # create model\n",
    "    model = simpleNN(dim_in=64, dim_out=4, hidden_dim=256, num_layers=8)\n",
    "    model = model.to(model)\n",
    "    model = DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for i in range(0, 10000):\n",
    "        input_ = next(dataloader)\n",
    "        x, y = input_.x, input_.y\n",
    "\n",
    "        out = model(x)\n",
    "        loss = criterion(y, out)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        '''\n",
    "        ...\n",
    "        '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
