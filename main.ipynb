{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Distributed Data parallel(DDP) processing tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import pytorch module for do DDP processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    args.world_size = n_gpus\n",
    "\n",
    "    if args.use_ddp: # a flag for using Pytorch DDP\n",
    "        args.world_size = n_gpus * args.world_size\n",
    "        args.num_workers = n_gpus * 4 # it's common to multiply 4 with how many gpu you have\n",
    "        args.batch_size = n_gpus * args.batch_size # to split batch per each gpu\n",
    "        args.val_batch_size = n_gpus * args.val_batch_size # same above\n",
    "        mp.spawn(main_worker, nprocs=n_gpus, args=(n_gpus, args)) # pytorch multiprocessing spawn\n",
    "    else: # a flag for not using Pytorch DDP\n",
    "        args.gpu = 0 # set first gpu id\n",
    "        main_worker(args.gpu, n_gpus, args)\n",
    "\n",
    "def main_worker(gpu, ngpus_per_node, args):\n",
    "    args.gpu = gpu\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "\n",
    "    if args.gpu is not None:\n",
    "        print(\"Use GPU: {} for training\".format(args.gpu))\n",
    "\n",
    "    if args.distributed:\n",
    "        if args.is_distributed:\n",
    "            args.rank = args.rank * ngpus_per_node + gpu\n",
    "        dist.init_process_group(backend='nccl', \n",
    "                                init_method='tcp://127.0.0.1:88', #127.0.0.1:23456\n",
    "                                world_size=args.world_size, \n",
    "                                rank=args.rank)\n",
    "\n",
    "    args.flag = True if args.is_distributed == False or args.gpu == 0 else False\n",
    "\n",
    "    if args.flag:\n",
    "        print(args)\n",
    "    solver = Solver(args)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
