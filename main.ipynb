{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Distributed Data parallel(DDP) processing tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import pytorch module for do DDP processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    args.world_size = n_gpus\n",
    "\n",
    "    if args.use_ddp: # a flag for using Pytorch DDP\n",
    "        args.world_size = n_gpus * args.world_size\n",
    "        args.num_workers = n_gpus * 4 # it's common to multiply 4 with how many gpu you have\n",
    "        args.batch_size = n_gpus * args.batch_size # to split batch per each gpu\n",
    "        args.val_batch_size = n_gpus * args.val_batch_size # same above\n",
    "        mp.spawn(main_worker, nprocs=n_gpus, args=(n_gpus, args)) # pytorch multiprocessing spawn\n",
    "    else: # a flag for not using Pytorch DDP\n",
    "        args.gpu = 0 # set first gpu id\n",
    "        main_worker(args.gpu, n_gpus, args)\n",
    "\n",
    "def main_worker(gpu, ngpus_per_node, args):\n",
    "    args.gpu = gpu\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "\n",
    "    if args.gpu is not None:\n",
    "        print(\"Use GPU: {} for training\".format(args.gpu))\n",
    "\n",
    "    if args.distributed:\n",
    "        if args.is_distributed:\n",
    "            args.rank = args.rank * ngpus_per_node + gpu\n",
    "        dist.init_process_group(backend='nccl', # 'gloo', 'mpi', or 'nccl', check out their capabilities\n",
    "                                init_method='tcp://127.0.0.1:88', #{localhost}:OPENPORT\n",
    "                                world_size=args.world_size, \n",
    "                                rank=args.rank)\n",
    "\n",
    "    # load dataset\n",
    "    transform = transforms.Compose([transforms.Resize([256, 256]),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                                        std=[0.5, 0.5, 0.5])])\n",
    "    dataset = ImageFolder(\"your directory\", transform=transform)\n",
    "\n",
    "    sampler = DistributedSampler(dataset=dataset, shuffle=True) # split train data per each process\n",
    "    dataloader = DataLoader(dataset, \n",
    "                            batch_size=int(args.batch_size / args.world_size),\n",
    "                            shuffle=False,\n",
    "                            num_workers=int(args.num_workers / args.world_size)\n",
    "                            sampler=sampler,\n",
    "                            pin_memory=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
